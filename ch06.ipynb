{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsutsumi-ozro/NLP-100knocks/blob/main/ch06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5gRq-cQkgag"
      },
      "source": [
        "## 第6章: 機械学習\n",
        "本章では，Fabio Gasparetti氏が公開している[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)を用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJLaFpy_kgai"
      },
      "source": [
        "### 50. データの入手・整形\n",
        "[News Aggregator Data Set](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)をダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
        "\n",
        "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
        "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
        "3. 抽出された事例をランダムに並び替える．\n",
        "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
        "\n",
        "\n",
        "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "uygqX28IpQQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip /content/NewsAggregatorDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3N43xptkpVi",
        "outputId": "59f6092e-1801-4eea-d2d4-38a899a53918"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-05 03:05:13--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  24.8MB/s    in 1.1s    \n",
            "\n",
            "2023-01-05 03:05:14 (24.8 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
            "\n",
            "Archive:  /content/NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "replace __MACOSX/._2pageSessions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "replace __MACOSX/._newsCorpora.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "replace __MACOSX/._readme.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2, 3"
      ],
      "metadata": {
        "id": "2m5Zfl-rpRbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "lWzctLfplexu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
        "df = pd.read_csv('/content/newsCorpora.csv', header=None, sep='\\t', names=columns)\n",
        "\n",
        "df = df[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])]\n",
        "# df.query(\"PUBLISHER in ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\")\n",
        "df = df.sample(frac=1, random_state=0)"
      ],
      "metadata": {
        "id": "LLv_Ga3dlp_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://note.nkmk.me/python-pandas-random-sort-shuffle/"
      ],
      "metadata": {
        "id": "V7Q1O6p4o94f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "ADy9rNlIuy7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#一応u++さんがlabel encodingしているので先にしておく  -> https://upura.hatenablog.com/entry/2020/07/25/235334\n",
        "use_cols = ['TITLE', 'CATEGORY']\n",
        "\n",
        "X = df[['TITLE', 'CATEGORY']].copy()\n",
        "X['CATEGORY'] = X['CATEGORY'].map({'b':0, 'e':1, 't':2, 'm':3})\n",
        "y = df['CATEGORY']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=0)\n",
        "\n",
        "X_train.to_csv('./train.txt', sep='\\t', index=False, header=None)\n",
        "X_valid.to_csv('./valid.txt', sep='\\t', index=False, header=None)\n",
        "X_test.to_csv('./test.txt', sep='\\t', index=False, header=None)"
      ],
      "metadata": {
        "id": "L5dRpwqDqxMX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmquXb0rkgai"
      },
      "source": [
        "### 51. 特徴量抽出\n",
        "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<見た解答>\n",
        "https://upura.hatenablog.com/entry/2020/07/26/000124<br>\n",
        "\n",
        "CountVectorizer<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer<br>\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction<br>\n",
        "\n",
        "joblib.dump<br>\n",
        "https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html"
      ],
      "metadata": {
        "id": "t1NlwA3_xEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "#CountVectorizer -> token数の行列に変換する\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "X_train['TMP'] = 'train'\n",
        "X_test['TMP'] = 'test'\n",
        "X_valid['TMP'] = 'valid'\n",
        "\n",
        "data = pd.concat([X_train, X_valid, X_test]).reset_index(drop=True)\n",
        "# このtoken_patternは何？\n",
        "vectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
        "bag = vectorizer.fit_transform(data['TITLE'])\n",
        "data = pd.concat([data, pd.DataFrame(bag.toarray())], axis=1)\n",
        "\n",
        "joblib.dump(vectorizer.vocabulary_, 'vocabulary_.joblib')\n",
        "\n",
        "X_train_feature = data.query('TMP==\"train\"').drop(use_cols+['TMP'], axis=1)\n",
        "X_valid_feature = data.query('TMP==\"valid\"').drop(use_cols+['TMP'], axis=1)\n",
        "X_test_feature = data.query('TMP==\"test\"').drop(use_cols+['TMP'], axis=1)\n",
        "\n",
        "X_train_feature.to_csv('./train.feature.txt', sep='\\t', index=False, header=None)\n",
        "X_valid_feature.to_csv('./valid.feature.txt', sep='\\t', index=False, header=None)\n",
        "X_test_feature.to_csv('./test.feature.txt', sep='\\t', index=False, header=None)"
      ],
      "metadata": {
        "id": "ZbjBDcbrw5rF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVGmQ34-kgaj"
      },
      "source": [
        "### 52. 学習\n",
        "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2Tpa9x55Mmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMoloMzGkgaj"
      },
      "source": [
        "### 53. 予測\n",
        "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVzGhabAkgaj"
      },
      "source": [
        "### 54. 正解率の計測\n",
        "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1K5qVD6kgaj"
      },
      "source": [
        "### 55. 混同行列の作成\n",
        "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf7WmM2xkgaj"
      },
      "source": [
        "### 56. 適合率，再現率，F1スコアの計測\n",
        "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghud9cJakgaj"
      },
      "source": [
        "### 57. 特徴量の重みの確認\n",
        "52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYaeF-Jhkgak"
      },
      "source": [
        "### 58. 正則化パラメータの変更\n",
        "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVXCU6makgak"
      },
      "source": [
        "### 59. ハイパーパラメータの探索\n",
        "学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bo4svrx1kgak"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}